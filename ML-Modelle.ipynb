{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Based Activity Recoginition \n",
    "Challenge: cdl1 - Sensor based Activity Recognition  \n",
    "Team: Lea Bütler, Manjavy Kirupa, Etienne Roulet, Si Ben Tran  \n",
    "\n",
    "Aufgabe: ML Modell erstellen\n",
    "\n",
    "Hier in diesem Notebook erstellen wir unsere Machine Learning Modelle.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# Preprocessing \n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "\n",
    "# ML Models\n",
    "## import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import metrics \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# import wandb\n",
    "import wandb\n",
    "!wandb login 36a80c1d4728497df56a7605464c8a2263baa824\n",
    "print(\"Successfull wandb login: \", wandb.login())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Einlesen der unprocessierten Daten\n",
    "- Einlesen der getrimmten und aggregierten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: Alle_Messungen.csv (Raw Data) einlesen\n",
    "raw_unprocessed_data = pd.read_csv(\"../Sensor_Data-Wrangling-und-EDA/Alle_Messungen.csv\", index_col=0)\n",
    "data = pd.read_csv(\"../Sensor_Data-Wrangling-und-EDA/data_trimmed_aggregated_windows_size_5s.csv\")\n",
    "# display head \n",
    "display(\"raw_unprocessed_data \" + str(raw_unprocessed_data.shape), raw_unprocessed_data.head())\n",
    "display(\"data \" + str(data.shape), data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing der Daten für Baseline Modell\n",
    "\n",
    "Für das Baseline Modell werden die Daten wie folgt verarbeitet:\n",
    "\n",
    "- Entfernen der Zeilen, die NaN Werte aufweisen\n",
    "- Die Anzahl Observationen gleichmässig auf die minimale Anzahl Observationen pro Klasse sampeln\n",
    "- Die Daten werden in Trainings und Validierungsdaten gesplittet, dabei sollten die Anzahl der Werte in 'class' gleichmaessig verteilt sein\n",
    "- Die Verteilung der Klassen wird mittels einem Barplot gezeigt\n",
    "- Das erstellen eines Logistischen Regression Modell fuer multiple Klassen, mittels sklearn\n",
    "- Berechnung der Accuracy sowie Visualisierung der Confusion Matrix\n",
    "\n",
    "Fragen: \n",
    "\n",
    "- Warum wurde hier nicht nach \"id_combined\" gesplittet?  \n",
    "Hier handelt es sich um die unverarbeiteten Daten, also die Daten, die wir weder getrimmt (5 Sekunden Schnitt) noch aggregiert haben. Die Daten sind also noch in der Form, wie sie vom Sensor kommen. Da es sich hierbei um ein Baseline Model handelt, haben wir uns dazu entschieden, die Daten nicht nach \"id_combined\" zu splitten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na rows from raw_unprocessed_data\n",
    "raw_unprocessed_data = raw_unprocessed_data.dropna(axis = 0)\n",
    "\n",
    "# drop uneccessary columns for the baseline modell\n",
    "baseline_data = raw_unprocessed_data.drop(['time', 'id', 'user', 'id_combined', 'system'], axis = 1)\n",
    "\n",
    "# get the number of observation for each class and select the minimum\n",
    "min_class = baseline_data['class'].value_counts().min()\n",
    "\n",
    "# Select the min_class of each class to get an equal number of observations for each class\n",
    "baseline_data = baseline_data.groupby('class').sample(n = min_class, random_state=42)\n",
    "\n",
    "# split the data into baselin_train_X, baseline_train_y, baseline_test_X, baseline_test_y \n",
    "baseline_train_X, baseline_val_X, baseline_train_y, baseline_val_y = train_test_split(baseline_data.drop('class', axis = 1), baseline_data['class'], test_size = 0.2, random_state = 42)\n",
    "\n",
    "# plot the class distribution of the baselin_data, baseline_train_y and baseline_test_y in 3 subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "# plot the class distribution of the baseline_data\n",
    "baseline_data['class'].value_counts().plot(kind = \"bar\", ax = ax1, color = \"lightblue\", edgecolor = \"black\", linewidth = 1)\n",
    "ax1.set_xlabel(\"Class\")\n",
    "ax1.set_ylabel(\"Number of Observations\")\n",
    "ax1.set_title(\"Number of Observations per Class in Baseline Data\")\n",
    "\n",
    "# plot the class distribution of the baseline_train_y\n",
    "baseline_train_y.value_counts().plot(kind = \"bar\", ax = ax2, color = \"lightblue\", edgecolor = \"black\", linewidth = 1)\n",
    "ax2.set_xlabel(\"Class\")\n",
    "ax2.set_ylabel(\"Number of Observations\")\n",
    "ax2.set_title(\"Number of Observations per Class in Train Data\")\n",
    "\n",
    "# plot the class distribution of the baseline_test_y\n",
    "baseline_val_y.value_counts().plot(kind = \"bar\", ax = ax3, color = \"lightblue\", edgecolor = \"black\", linewidth = 1)\n",
    "ax3.set_xlabel(\"Class\")\n",
    "ax3.set_ylabel(\"Number of Observations\")\n",
    "ax3.set_title(\"Number of Observations per Class in Validate Data\")\n",
    "\n",
    "# set the title of the figure and optimize the layout of the subplots\n",
    "fig.suptitle(\"Class Distribution of Baseline Data, Train and Test Data\", fontsize = 16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Preprocessing der Daten für die ML-Modelle\n",
    "\n",
    "- Damit wir ein Data Leakage vermeiden, splitten wir die Daten aufgrund von id_combined, somit stellen wir sicher, dass die Daten, die wir für das Training verwenden, nicht in der Validierung verwendet werden.\n",
    "- Weil wir die Accuracy Metrik haben, ist es uns wichtig, dass die \"Class\" gleichverteilt ist sowohl beim Training als auch beim Test Datensatz. Die Funktin ermoeglicht es uns die Daten zu splitten, sodass die Verteilung der Klassen gleich ist. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def preprocess_data(data, test_size=0.2, random_state=42, make_plot=False, undersample=False, oversample=False):\n",
    "    # Split data into train and validation sets using GroupShuffleSplit\n",
    "    df_train_idx, df_val_idx = next(iter(sklearn.model_selection.GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state).split(data, groups=data['id_combined'])))\n",
    "    df_train_ = data.iloc[df_train_idx]\n",
    "    df_val_ = data.iloc[df_val_idx]\n",
    "\n",
    "    # Undersample if specified, else oversample if specified, else keep original dataset\n",
    "    if undersample and not oversample:\n",
    "        print(\"undersample dataset\")\n",
    "        df_train = df_train_.groupby('class').sample(n = df_train_['class'].value_counts().min(), random_state=random_state)\n",
    "        df_val = df_val_.groupby('class').sample(n = df_val_['class'].value_counts().min(), random_state=random_state)\n",
    "    elif oversample and not undersample:\n",
    "        print(\"oversample dataset\")\n",
    "        df_train = df_train_.groupby('class').sample(n = df_train_['class'].value_counts().max(), replace=True, random_state=random_state)\n",
    "        df_val = df_val_.groupby('class').sample(n = df_val_['class'].value_counts().max(), replace=True, random_state=random_state)\n",
    "\n",
    "        #df_train = df_train_.groupby('class').apply(lambda x: x.sample(df_train_['class'].value_counts().max(), replace=True, random_state=42))\n",
    "        #df_val = df_val_.groupby('class').apply(lambda x: x.sample(df_val_['class'].value_counts().max(), replace=True, random_state=42))\n",
    "    else:\n",
    "        print(\"no undersampling or oversampling\")\n",
    "        df_train = df_train_\n",
    "        df_val = df_val_\n",
    "\n",
    "    # Plot class distribution if specified\n",
    "    if make_plot:\n",
    "        # create counterplot for the class distribution of train and test data\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "        # plot the class distribution of the train data\n",
    "        df_train['class'].value_counts().plot(kind = \"bar\", ax = ax1, color = \"lightblue\", edgecolor = \"black\", linewidth = 1)\n",
    "        ax1.set_xlabel(\"Class\")\n",
    "        ax1.set_ylabel(\"Number of Observations\")\n",
    "        ax1.set_title(\"Number of Observations per Class in Train Data\")\n",
    "        for p in ax1.patches:\n",
    "            ax1.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "        # plot the class distribution of the test data\n",
    "        df_val['class'].value_counts().plot(kind = \"bar\", ax = ax2, color = \"lightblue\", edgecolor = \"black\", linewidth = 1)\n",
    "        ax2.set_xlabel(\"Class\")\n",
    "        ax2.set_ylabel(\"Number of Observations\")\n",
    "        ax2.set_title(\"Number of Observations per Class in Validate Data\")\n",
    "        for p in ax2.patches:\n",
    "            ax2.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "        # set the title of the figure and optimize the layout of the subplots\n",
    "        fig.suptitle(\"Class Distribution of Train and Validate Dataz\\n undersample: \" + str(undersample) + \" ,oversample: \" + str(oversample), fontsize = 16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # cols that should be dropped from the dataset\n",
    "    cols_to_drop = ['time', 'Accelerometer_x', 'Accelerometer_y', 'Accelerometer_z', 'Gyroscope_x', 'Gyroscope_y', 'Gyroscope_z', 'Magnetometer_x',\n",
    "                    'Magnetometer_y', 'Magnetometer_z', 'Orientation_qx', 'Orientation_qy', 'Orientation_qz', 'id', 'user', 'id_combined', 'system']\n",
    "    \n",
    "    # Drop columns\n",
    "    df_train = df_train.drop(cols_to_drop, axis=1)\n",
    "    df_val = df_val.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    # drop rows with NaN Values (cause from the export_aggregated_df(concat_only=True) function, there exists rows with NaN values)\n",
    "    df_train = df_train.dropna()\n",
    "    df_val = df_val.dropna()\n",
    "\n",
    "    # Creata X and y \n",
    "    y_train = df_train['class']\n",
    "    X_train = df_train.drop('class', axis=1)\n",
    "    y_val = df_val['class']\n",
    "    X_val = df_val.drop('class', axis=1)\n",
    "\n",
    "    # Return preprocessed data\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data function with undersampling\n",
    "preprocess_data(data, test_size=0.2, random_state=3, make_plot=True, undersample=False, oversample=True)\n",
    "# preprocess data function with oversampling\n",
    "preprocess_data(data, test_size=0.3, random_state=5, make_plot=True, undersample=True, oversample=False)\n",
    "# preprocess data function with no undersampling or oversampling\n",
    "preprocess_data(data, test_size=0.3, random_state=7, make_plot=True, undersample=False, oversample=False)\n",
    "\n",
    "print(\"Oversample and Undersampling Data Works\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Baseline Modell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logistic regression model for multiclass classification with sklearn\n",
    "logistic_regression = LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\", max_iter = 1000)\n",
    "\n",
    "# fit the logistic regression model\n",
    "logistic_regression.fit(X = baseline_train_X, y = baseline_train_y)\n",
    "\n",
    "# predict the classes for the val data\n",
    "baseline_val_y_pred = logistic_regression.predict(X = baseline_val_X)\n",
    "\n",
    "# calculate the accuracy of the logistic regression model with train data and val data\n",
    "baseline_val_accuracy = accuracy_score(y_true = baseline_val_y, y_pred = baseline_val_y_pred)\n",
    "\n",
    "# create a confusion matrix display\n",
    "baseline_confmatrix = confusion_matrix(y_true = baseline_val_y, y_pred = baseline_val_y_pred)\n",
    "\n",
    "# plot the confusion matrix of the logistic regression model\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=baseline_confmatrix, display_labels=logistic_regression.classes_)\n",
    "disp.plot(ax = ax, cmap = \"Blues\", values_format = \"d\", xticks_rotation = 'vertical')\n",
    "plt.title(\"Confusion Matrix of Logistic Regression Baseline Model \\n Accuracy Score of: \" + str(round(baseline_val_accuracy, 4)), fontsize = 20, y = 1.01)\n",
    "plt.xlabel(\"Predicted Class\", fontsize = 15)\n",
    "plt.ylabel(\"True Class\", fontsize = 15)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Baseline Modell ist ein Logistisches Regression Modell. Wir haben uns für dieses Modell entschieden, da es einfach zu implementieren ist und wir so einen Vergleich zu den anderen Machine Learning Modellen erstellen können."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modell-01 - Decision Tree\n",
    "## Model trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep config for decision tree\n",
    "sweep_config_dt = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'accuracy_val',\n",
    "        'goal': 'maximize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        'criterion': {\n",
    "            'values': ['gini']\n",
    "        },\n",
    "        'splitter': {\n",
    "            'values': ['best']\n",
    "        },\n",
    "        'max_depth': {\n",
    "            'values': [5]\n",
    "        },\n",
    "        'min_samples_split': {\n",
    "            'values': [5]\n",
    "        },\n",
    "        'min_samples_leaf': {\n",
    "            'values': [10]\n",
    "        },\n",
    "        'max_features': {\n",
    "            'values': ['auto']\n",
    "        },\n",
    "        'test_size': {\n",
    "            'values': [0.2]\n",
    "        },\n",
    "        'random_state': {\n",
    "            'min': 1,\n",
    "            'max': 10000,\n",
    "        },\n",
    "        'undersample': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'oversample': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'class_weight': {\n",
    "            'values': ['balanced']\n",
    "        },\n",
    "        'ccp_alpha': {\n",
    "            'values': [0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config_dt, project=\"01-decision-tree\")\n",
    "\n",
    "# Define sweep function\n",
    "def sweep_dt_train():\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"01-decision-tree\")\n",
    "\n",
    "    # Get hyperparameters\n",
    "    criterion = wandb.config.criterion\n",
    "    splitter = wandb.config.splitter\n",
    "    min_samples_split = wandb.config.min_samples_split\n",
    "    max_depth = wandb.config.max_depth\n",
    "    min_samples_leaf = wandb.config.min_samples_leaf\n",
    "    max_features = wandb.config.max_features\n",
    "    class_weight = wandb.config.class_weight\n",
    "    ccp_alpha = wandb.config.ccp_alpha\n",
    "\n",
    "    # Parametres for preprocess data \n",
    "    test_size = wandb.config.test_size\n",
    "    random_state = np.round(wandb.config.random_state, 1)\n",
    "    undersample = wandb.config.undersample\n",
    "    oversample = wandb.config.oversample\n",
    "\n",
    "    # load data and process it \n",
    "    X_train, y_train, X_val, y_val = preprocess_data(data, \n",
    "                                                     test_size=test_size, \n",
    "                                                     random_state=random_state, \n",
    "                                                     undersample=undersample, \n",
    "                                                     oversample=oversample)\n",
    "    \n",
    "    # create a decision tree model\n",
    "    dt_model = DecisionTreeClassifier(criterion=criterion,\n",
    "                                      splitter=splitter,\n",
    "                                      max_depth=max_depth,\n",
    "                                      min_samples_split=min_samples_split,\n",
    "                                      min_samples_leaf=min_samples_leaf,\n",
    "                                      max_features=max_features,\n",
    "                                      class_weight=class_weight,\n",
    "                                      ccp_alpha=ccp_alpha)\n",
    "    \n",
    "    # fit the decision tree model\n",
    "    dt_model.fit(X_train, y_train)\n",
    "\n",
    "    # predict the classes for the val data\n",
    "    prediction_val = dt_model.predict(X_val)\n",
    "    prediction_train = dt_model.predict(X_train)\n",
    "\n",
    "    # calculate the accuracy of the decision tree model with train and val data\n",
    "    accuracy_val = accuracy_score(y_true = y_val, y_pred = prediction_val)\n",
    "    accuracy_train = accuracy_score(y_true = y_train, y_pred = prediction_train)\n",
    "\n",
    "    precision_val = precision_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    precision_train = precision_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")\n",
    "\n",
    "    recall_val = recall_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    recall_train = recall_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")\n",
    "\n",
    "    f1_val = f1_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    f1_train = f1_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")    \n",
    "\n",
    "    # log metrics\n",
    "    wandb.log({'accuracy_val': accuracy_val,\n",
    "                'accuracy_train': accuracy_train,\n",
    "                'precision_val': precision_val,\n",
    "                'precision_train': precision_train,\n",
    "                'recall_val': recall_val,\n",
    "                'recall_train': recall_train,\n",
    "                'f1_val': f1_val,\n",
    "                'f1_train': f1_train})\n",
    "    \n",
    "# Run sweep\n",
    "wandb.agent(sweep_id, function=sweep_dt_train, count=100)\n",
    "wandb.finish()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bestes Modell\n",
    "Nachdem mit Weights & Bias verschiedene Modelle trainiert und getestet wurden, konnte ein Accuracy Score von 0.8 - 0.9 erreicht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "decision_tree_X_train, decision_tree_y_train, decision_tree_X_val, decision_tree_y_val = preprocess_data(data, test_size=0.2, random_state=7885, make_plot=False, undersample=False, oversample=False)\n",
    "\n",
    "# create a logistic regression model for multiclass classification with sklearn\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=10, max_features=\"auto\", min_samples_leaf=20, min_samples_split=15, splitter=\"best\")\n",
    "\n",
    "# fit the logistic regression model\n",
    "decision_tree.fit(X = decision_tree_X_train, y = decision_tree_y_train)\n",
    "\n",
    "# predict the classes for the val data\n",
    "decision_tree_val_y_pred = decision_tree.predict(X = decision_tree_X_val)\n",
    "\n",
    "# calculate the accuracy of the logistic regression model with train data and val data\n",
    "decision_tree_val_accuracy = accuracy_score(y_true = decision_tree_y_val, y_pred = decision_tree_val_y_pred)\n",
    "\n",
    "# create a confusion matrix display\n",
    "decision_tree_confmatrix = confusion_matrix(y_true = decision_tree_y_val, y_pred = decision_tree_val_y_pred)\n",
    "\n",
    "# plot the confusion matrix of the logistic regression model\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=decision_tree_confmatrix, display_labels=decision_tree.classes_)\n",
    "disp.plot(ax = ax, cmap = \"Blues\", values_format = \"d\", xticks_rotation = 'vertical')\n",
    "plt.title(\"Confusion Matrix of Decision Tree best Model \\n Accuracy Score of: \" + str(round(decision_tree_val_accuracy, 4)), fontsize = 20, y = 1.01)\n",
    "plt.xlabel(\"Predicted Class\", fontsize = 15)\n",
    "plt.ylabel(\"True Class\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "Für das Optimieren der Machine Learning Modelle wurde die Plattform Weights and Biases verwendet, um Grid Searches auszuführen und diese zu visualisieren.Durch die Visualisierungen können die trainierten modelle besser verglichen und die besten Parameter ausgewählt werden.\n",
    "Total wurden über 400 Modelle trainiert und getestet und validiert:\n",
    "![alt text](images/01_decision_tree/01_decision_tree_all.png \"Decision Tree - All Runs\")\n",
    "\n",
    "Zu beginn wurden standard Modellparameter mit üblichen Values getestet. Diese erzielten validation accuracies von 0.29 - 0.93. Das Modell welches am besten performte wurde jedoch mit einer Testsize von 0.1 trainiert, was zu klein ist und daher nicht aussagekräftig ist.\n",
    "![alt text](images/01_decision_tree/02_desicion_tree_run_1.png \"Decision Tree - Run 01\")\n",
    "\n",
    "Die Parameter, welche im ersten run die besten ergebnssse erzielten wurden übernommen und schlecht performte values wurden entfernt.\n",
    "![alt text](images/01_decision_tree/03_decision_tree_run_2.png \"Decision Tree - Run 02\")\n",
    "\n",
    "Auffallend war nun, dass die Testsize 0.1 zu klein gewählt ist, weshalb diese auf 0.2 erhöht wurde.\n",
    "![alt text](images/01_decision_tree/04_decision_tree_run_3.png \"Decision Tree - Run 03\")\n",
    "\n",
    "Erneut wurden nur die gut perfomenden Values übernommen und die Testsuze auf 0.2 fixiert.\n",
    "![alt text](images/01_decision_tree/05_decision_tree_run_4.png \"Decision Tree - Run 04\")\n",
    "\n",
    "Die besten Parameter wurden übernommen und diesmal nur gegenüber des Randomstates von train und test split getestet. Der Decision Tree hängt sehr stark vom Split der Daten ab.\n",
    "![alt text](images/01_decision_tree/06_decision_tree_run_5.png \"Decision Tree - Run 05\")\n",
    "\n",
    "Auch mit dieser Abhängigkeit des Radomstates konnte ein gutes ergebnis erzielt werden. Allerdings ist auffallend, dass die Train Accuracy gegenüber der Test Accuracy sehr hoch ist, was auf ein overfitting hindeutet. Aus diesem Grund wurden die Parameter Class Weight und \n",
    "![alt text](images/01_decision_tree/07_decision_tree_run_6.png \"Decision Tree - Run 06\")\n",
    "\n",
    "![alt text](images/01_decision_tree/08_decision_tree_run_7.png \"Decision Tree - Run 07\")\n",
    "\n",
    "Nach der Evaluierung konnten einige Erkenntnisse gewonnen werden:\n",
    "- zu wenig Daten\n",
    "- Scores sind extrem abhängig des Randomstates von train und test split\n",
    "- overfit\n",
    "- zu kleine testsize angenommen (danach min 0.2)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell-02 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n",
    "# Define sweep config random forest\n",
    "sweep_config_rf = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'accuracy_val',\n",
    "        'goal': 'maximize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        'n_estimators': {\n",
    "            'min': 50,\n",
    "            'max': 200,\n",
    "        },\n",
    "        'criterion': {\n",
    "            'values': ['gini', 'entropy']\n",
    "        },\n",
    "        'max_depth': {\n",
    "            'min': 5,\n",
    "            'max': 10,\n",
    "        },\n",
    "        'min_samples_split': {\n",
    "            'min': 2,\n",
    "            'max': 10,\n",
    "        },\n",
    "        'min_samples_leaf': {\n",
    "            'min': 1,\n",
    "            'max': 10,\n",
    "        },\n",
    "        'max_features': {\n",
    "            'values': ['auto', 'sqrt', 'log2'],\n",
    "        },\n",
    "        'bootstrap': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'test_size': {\n",
    "            'values': [0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'random_state': {\n",
    "            'min': 1,\n",
    "            'max': 10000,\n",
    "        },\n",
    "        'undersample': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'oversample': {\n",
    "            'values': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config_rf, project=\"02-random-forest\")\n",
    "\n",
    "# Define sweep function\n",
    "def sweep_rf_train():\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"02-random-forest\")\n",
    "\n",
    "    # Get hyperparameters\n",
    "    n_estimators = wandb.config.n_estimators\n",
    "    criterion = wandb.config.criterion\n",
    "    max_depth = wandb.config.max_depth\n",
    "    min_samples_split = wandb.config.min_samples_split\n",
    "    min_samples_leaf = wandb.config.min_samples_leaf\n",
    "    max_features = wandb.config.max_features\n",
    "    bootstrap = wandb.config.bootstrap\n",
    "\n",
    "    # Parametres for preprocess data \n",
    "    test_size = wandb.config.test_size\n",
    "    random_state = np.round(wandb.config.random_state, 1)\n",
    "    undersample = wandb.config.undersample\n",
    "    oversample = wandb.config.oversample\n",
    "\n",
    "    # load data and process it \n",
    "    X_train, y_train, X_val, y_val = preprocess_data(data, \n",
    "                                                     test_size=test_size, \n",
    "                                                     random_state=random_state, \n",
    "                                                     undersample=undersample, \n",
    "                                                     oversample=oversample)\n",
    "    \n",
    "    # create a random forest model\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                       criterion=criterion,\n",
    "                                       max_depth=max_depth,\n",
    "                                       min_samples_split=min_samples_split,\n",
    "                                       min_samples_leaf=min_samples_leaf,\n",
    "                                       max_features=max_features,\n",
    "                                       bootstrap=bootstrap)\n",
    "    \n",
    "    # fit the random forest model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # predict the classes for the val data\n",
    "    prediction_val = rf_model.predict(X_val)\n",
    "    prediction_train = rf_model.predict(X_train)\n",
    "\n",
    "    # calculate the accuracy of the random forest model with train and val data\n",
    "    accuracy_val = accuracy_score(y_true = y_val, y_pred = prediction_val)\n",
    "    accuracy_train = accuracy_score(y_true = y_train, y_pred = prediction_train)\n",
    "\n",
    "    precision_val = precision_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    precision_train = precision_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")\n",
    "\n",
    "    recall_val = recall_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    recall_train = recall_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")\n",
    "\n",
    "    f1_val = f1_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    f1_train = f1_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")    \n",
    "\n",
    "    # log metrics\n",
    "    wandb.log({'accuracy_val': accuracy_val,\n",
    "                'accuracy_train': accuracy_train,\n",
    "                'precision_val': precision_val,\n",
    "                'precision_train': precision_train,\n",
    "                'recall_val': recall_val,\n",
    "                'recall_train': recall_train,\n",
    "                'f1_val': f1_val,\n",
    "                'f1_train': f1_train})\n",
    "    \n",
    "# Run sweep\n",
    "wandb.agent(sweep_id, function=sweep_rf_train, count=100)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit zu Modell-02 - Random Forest\n",
    "\n",
    "blabla\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell-03 - KNN\n",
    "## Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: b9fdeolz\n",
      "Sweep URL: https://wandb.ai/fhnw-cdl1/03-knn/sweeps/b9fdeolz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x768r3m3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talgorithm: auto\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_neighbors: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toversample: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 7155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_size: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tundersample: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweights: distance\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/leabuetler/git_repos/06_Semester/cdl1/Sensor-Klassifikation-ohne-Deep-Learning/wandb/run-20230508_142752-x768r3m3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fhnw-cdl1/03-knn/runs/x768r3m3' target=\"_blank\">solar-sweep-1</a></strong> to <a href='https://wandb.ai/fhnw-cdl1/03-knn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/fhnw-cdl1/03-knn/sweeps/b9fdeolz' target=\"_blank\">https://wandb.ai/fhnw-cdl1/03-knn/sweeps/b9fdeolz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fhnw-cdl1/03-knn' target=\"_blank\">https://wandb.ai/fhnw-cdl1/03-knn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/fhnw-cdl1/03-knn/sweeps/b9fdeolz' target=\"_blank\">https://wandb.ai/fhnw-cdl1/03-knn/sweeps/b9fdeolz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fhnw-cdl1/03-knn/runs/x768r3m3' target=\"_blank\">https://wandb.ai/fhnw-cdl1/03-knn/runs/x768r3m3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no undersampling or oversampling\n"
     ]
    }
   ],
   "source": [
    "# Define sweep config for KNN\n",
    "sweep_config_knn = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'accuracy_val',\n",
    "        'goal': 'maximize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        'n_neighbors': {\n",
    "            'values': [2, 3, 5]\n",
    "        },\n",
    "        'weights': {\n",
    "            'values': ['uniform', 'distance']\n",
    "        },\n",
    "        'algorithm': {\n",
    "            'values': ['auto']\n",
    "        },\n",
    "        'p': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'test_size': {\n",
    "            'values': [0.2]\n",
    "        },\n",
    "        'random_state': {\n",
    "            'min': 1,\n",
    "            'max': 10000,\n",
    "        },\n",
    "        'undersample': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'oversample': {\n",
    "            'values': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config_knn, project=\"03-knn\")\n",
    "\n",
    "# Define sweep function\n",
    "def sweep_knn_train():\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"03-knn\")\n",
    "\n",
    "    # Get hyperparameters\n",
    "    n_neighbors = wandb.config.n_neighbors\n",
    "    weights = wandb.config.weights\n",
    "    algorithm = wandb.config.algorithm\n",
    "    p = wandb.config.p\n",
    "\n",
    "    # Parametres for preprocess data \n",
    "    test_size = wandb.config.test_size\n",
    "    random_state = np.round(wandb.config.random_state, 1)\n",
    "    undersample = wandb.config.undersample\n",
    "    oversample = wandb.config.oversample\n",
    "\n",
    "    # load data and process it \n",
    "    X_train, y_train, X_val, y_val = preprocess_data(data, \n",
    "                                                     test_size=test_size, \n",
    "                                                     random_state=random_state, \n",
    "                                                     undersample=undersample, \n",
    "                                                     oversample=oversample)\n",
    "    \n",
    "    # create a decision tree model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors,\n",
    "                                      weights=weights,\n",
    "                                      algorithm=algorithm,\n",
    "                                      p=p)\n",
    "    \n",
    "    # fit the decision tree model\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    # predict the classes for the val data\n",
    "    prediction_val = knn_model.predict(X_val)\n",
    "    prediction_train = knn_model.predict(X_train)\n",
    "\n",
    "    # calculate the accuracy of the decision tree model with train and val data\n",
    "    accuracy_val = accuracy_score(y_true = y_val, y_pred = prediction_val)\n",
    "    accuracy_train = accuracy_score(y_true = y_train, y_pred = prediction_train)\n",
    "\n",
    "    precision_val = precision_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    precision_train = precision_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")\n",
    "\n",
    "    recall_val = recall_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    recall_train = recall_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")\n",
    "\n",
    "    f1_val = f1_score(y_true = y_val, y_pred = prediction_val, average = \"weighted\")\n",
    "    f1_train = f1_score(y_true = y_train, y_pred = prediction_train, average = \"weighted\")    \n",
    "\n",
    "    # log metrics\n",
    "    wandb.log({'accuracy_val': accuracy_val,\n",
    "                'accuracy_train': accuracy_train,\n",
    "                'precision_val': precision_val,\n",
    "                'precision_train': precision_train,\n",
    "                'recall_val': recall_val,\n",
    "                'recall_train': recall_train,\n",
    "                'f1_val': f1_val,\n",
    "                'f1_train': f1_train})\n",
    "    \n",
    "# Run sweep\n",
    "wandb.agent(sweep_id, function=sweep_knn_train, count=100)\n",
    "wandb.finish()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bestes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "knn_X_train, knn_y_train, knn_X_val, knn_y_val = preprocess_data(data, test_size=0.2, random_state=7885, make_plot=False, undersample=False, oversample=False)\n",
    "\n",
    "# create a logistic regression model for multiclass classification with sklearn\n",
    "knn = DecisionTreeClassifier()  #TODO: add hyperparameters here\n",
    "\n",
    "# fit the logistic regression model\n",
    "knn.fit(X = knn_X_train, y = knn_y_train)\n",
    "\n",
    "# predict the classes for the val data\n",
    "knn_val_y_pred = knn.predict(X = knn_X_val)\n",
    "\n",
    "# calculate the accuracy of the logistic regression model with train data and val data\n",
    "knn_val_accuracy = accuracy_score(y_true = knn_y_val, y_pred = knn_val_y_pred)\n",
    "\n",
    "# create a confusion matrix display\n",
    "knn_confmatrix = confusion_matrix(y_true = knn_y_val, y_pred = knn_val_y_pred)\n",
    "\n",
    "# plot the confusion matrix of the logistic regression model\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=knn_confmatrix, display_labels=knn.classes_)\n",
    "disp.plot(ax = ax, cmap = \"Blues\", values_format = \"d\", xticks_rotation = 'vertical')\n",
    "plt.title(\"Confusion Matrix of KNN best Model \\n Accuracy Score of: \" + str(round(knn_val_accuracy, 4)), fontsize = 20, y = 1.01)\n",
    "plt.xlabel(\"Predicted Class\", fontsize = 15)\n",
    "plt.ylabel(\"True Class\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "blabla\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell-04 - SVM\n",
    "## Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bestes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "blabla\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell-05 - K-Means\n",
    "## Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bestes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "blabla\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell-06 - XGBoost\n",
    "## Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bestes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "blabla\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelle vergleichen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with the model names as key and the accuracy score as value\n",
    "model_accuracy_dict = {\"Logistic Regression Baseline Model\": baseline_val_accuracy}\n",
    "\n",
    "# create a dataframe from the dictionary\n",
    "model_accuracy_df = pd.DataFrame.from_dict(model_accuracy_dict, orient = \"index\", columns = [\"Accuracy Score\"])\n",
    "\n",
    "# sort dataframe by accuracy score\n",
    "model_accuracy_df = model_accuracy_df.sort_values(by = \"Accuracy Score\", ascending = False)\n",
    "\n",
    "# display the dataframe \n",
    "display(model_accuracy_df)\n",
    "\n",
    "# create a barplot for comparison \n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "model_accuracy_df.plot(kind = \"bar\", ax = ax, legend = False, color = \"lightblue\", edgecolor = \"black\", linewidth = 1)\n",
    "# add the accuracy score as text to the barplot\n",
    "plt.title(\"Accuracy Score comparison of ML Modells\", fontsize = 20, y = 1.01)\n",
    "plt.xlabel(\"Model\", fontsize = 15)\n",
    "plt.ylabel(\"Accuracy Score\", fontsize = 15)\n",
    "plt.xticks(rotation = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit zu ML Modelle \n",
    "\n",
    "blabla\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
